# battle: https://github.com/wsjeon/maddpg-rllib
group: "Gym"
name: "share/sac_hopper"

training:
  interface:
    type: "independent"
    population_size: -1
  config:
    # control the frequency of remote parameter update
    update_interval: 1000
    saving_interval: 10
    batch_size: 256
    optimizer: "Adam"
    actor_lr: 0.001
    critic_lr: 0.001
    sac_alpha: 0.2
    tau: 0.005  # soft update
    grad_norm_clipping: 5.0
    stopper: "simple_training"
    stopper_config:
      max_step: 1000000

rollout:
  type: "async"
  # stopper: "simple_rollout"
  # stopper_config:
  #   max_step: 10000
  # metric_type: "simple"
  stopper: "none"
  fragment_length: 1000
  num_episodes: 1
  episode_seg: 1
  test_num_episodes: 0
  test_episode_seg: 5
  terminate: "any"
  callback: "simultaneous"

env_description:
  creator: "Gym"
  config:
    env_id: "Hopper-v2"

algorithms:
  SAC:
    name: "SAC"
    model_config:
      actor:
        network: mlp
        layers:
          - units: 256
            activation: ReLU
          - units: 256
            activation: ReLU
        output:
          type: "gaussian"
          activation: False
      critic:
        network: mlp
        layers:
          - units: 256
            activation: ReLU
          - units: 256
            activation: ReLU
        output:
          activation: False
    custom_config:
      action_squash: True

global_evaluator:
  name: "generic"

dataset_config:
  episode_capacity: 1000000
  learning_start: 10000