# battle: https://github.com/wsjeon/maddpg-rllib
group: "Gym"
name: "share/bc_pendulum"

training:
  interface:
    type: "independent"
    population_size: -1
  config:
    # control the frequency of remote parameter update
    batch_size: 512
    optimizer: "Adam"
    lr: 0.01
    grad_norm_clipping: 2.0
    offline: False
    stopper: "simple_training"
    stopper_config:
      max_step: 50000

rollout:
  type: "async"
  stopper: "none"
  metric_type: "simple"
  fragment_length: 200
  num_episodes: 5
  episode_seg: 1
  test_num_episodes: 0
  test_episode_seg: 5
  terminate: "any"
  callback: "simultaneous"
  save_model: False

env_description:
  creator: "Gym"
  config:
    env_id: "Pendulum-v0"

algorithms:
  DDPG:
    name: "DDPG"
    model_config:
      actor:
        network: mlp
        layers:
          - units: 32
            activation: ReLU
          - units: 32
            activation: ReLU
        output:
          activation: False
      critic:
        network: mlp
        layers:
          - units: 32
            activation: ReLU
          - units: 32
            activation: ReLU
        output:
          activation: False
  ADVIRL:
    name: "ADVIRL"
    type: "GAIL" # choice in [GAIL, GAIL2, AIRL, FAIRL]
    model_config:
      reward_func:
        network: mlp
        layers:
          - units: 64
            activation: ReLU
          - units: 64
            activation: ReLU
        output:
          activation: False

global_evaluator:
  name: "generic"

dataset_config:
  episode_capacity: 100000
  extern:
    links:
      - name: "expert_data"
        path: "demos/Pendulum-v0/DDPG"
        write: False
    sample_rates: [1]
